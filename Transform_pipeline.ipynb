{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy import signal\n",
    "#from PyEMD import EMD\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "from python_speech_features import mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing cleaned dataset from cleaned_dataset.csv\n",
      "Data distribution visualization saved as 'data_distribution.png'\n"
     ]
    }
   ],
   "source": [
    "def load_and_analyze_data(file_path):\n",
    "    \"\"\"Load data from CSV file and perform initial analysis.\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(f\"Data loaded successfully. Shape: {data.shape}\")\n",
    "\n",
    "    # Display basic information about the dataset\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(data.info())\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(data.isnull().sum())\n",
    "\n",
    "    # Display basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(data.describe())\n",
    "\n",
    "    # Check for infinite values\n",
    "    print(\"\\nInfinite Values:\")\n",
    "    print(np.isinf(data.iloc[:, 1:]).sum())\n",
    "\n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst Few Rows:\")\n",
    "    print(data.head())\n",
    "\n",
    "    return data\n",
    "\n",
    "def clean_data(data, output_path='cleaned_dataset.csv'):\n",
    "    \"\"\"Clean the data by removing rows with NaN or infinite values and save the result.\"\"\"\n",
    "    original_shape = data.shape\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Remove rows with infinite values\n",
    "    data = data[~np.isinf(data.iloc[:, 1:]).any(axis=1)]\n",
    "    \n",
    "    cleaned_shape = data.shape\n",
    "    \n",
    "    print(f\"\\nData Cleaning:\")\n",
    "    print(f\"Original shape: {original_shape}\")\n",
    "    print(f\"Cleaned shape: {cleaned_shape}\")\n",
    "    print(f\"Rows removed: {original_shape[0] - cleaned_shape[0]}\")\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned dataset saved to {output_path}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def visualize_data_distribution(data):\n",
    "    \"\"\"Visualize the distribution of each feature.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(\"Data Distribution\")\n",
    "    \n",
    "    for i, column in enumerate(data.columns[1:]):\n",
    "        sns.histplot(data[column], kde=True, ax=axes[i//2, i%2])\n",
    "        axes[i//2, i%2].set_title(column)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data_distribution.png\")\n",
    "    plt.close()\n",
    "    print(\"Data distribution visualization saved as 'data_distribution.png'\")\n",
    "\n",
    "def main():\n",
    "    input_file = '../dataset/dataset.csv'\n",
    "    cleaned_file = 'cleaned_dataset.csv'\n",
    "\n",
    "    # Check if cleaned dataset already exists\n",
    "    if os.path.exists(cleaned_file):\n",
    "        print(f\"Loading existing cleaned dataset from {cleaned_file}\")\n",
    "        data = pd.read_csv(cleaned_file)\n",
    "    else:\n",
    "        # Load and analyze data\n",
    "        data = load_and_analyze_data(input_file)\n",
    "    \n",
    "        # Clean data\n",
    "        data = clean_data(data, cleaned_file)\n",
    "    \n",
    "    # Visualize data\n",
    "    visualize_data_distribution(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Shape: (479232, 4)\n",
      "Data preprocessing completed.\n",
      "DWT applied.\n",
      "STFT applied.\n",
      "PCA applied.\n",
      "ICA applied.\n",
      "WPT applied.\n",
      "S-transform applied.\n",
      "MFCC applied.\n",
      "Statistical features extracted.\n",
      "All transformed data saved to transformed_data_20240804_170250.xlsx\n",
      "All transformations applied and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global Variables for parameters\n",
    "DWT_PARAMS = {'wavelet': 'db4', 'level': 3}\n",
    "STFT_PARAMS = {'nperseg': 256, 'noverlap': 128}\n",
    "EMD_PARAMS = {'max_imf': 10}\n",
    "PCA_PARAMS = {'n_components': 2}\n",
    "ICA_PARAMS = {'n_components': 2}\n",
    "WPT_PARAMS = {'wavelet': 'db4', 'level': 3}\n",
    "MFCC_PARAMS = {'n_mfcc': 13}\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from CSV file.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(f\"Data loaded successfully. Shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess the data.\"\"\"\n",
    "    # Normalize data\n",
    "    data.iloc[:, 1:] = (data.iloc[:, 1:] - data.iloc[:, 1:].mean()) / data.iloc[:, 1:].std()\n",
    "    print(\"Data preprocessing completed.\")\n",
    "    return data\n",
    "\n",
    "def apply_dwt(data):\n",
    "    \"\"\"Apply Discrete Wavelet Transform.\"\"\"\n",
    "    transformed_data = []\n",
    "    for col in data.columns[1:]:\n",
    "        coeffs = pywt.wavedec(data[col], DWT_PARAMS['wavelet'], level=DWT_PARAMS['level'])\n",
    "        transformed_data.append(np.concatenate(coeffs))\n",
    "    print(\"DWT applied.\")\n",
    "    return np.array(transformed_data).T\n",
    "\n",
    "def apply_stft(data):\n",
    "    \"\"\"Apply Short-Time Fourier Transform.\"\"\"\n",
    "    transformed_data = []\n",
    "    for col in data.columns[1:]:\n",
    "        f, t, Zxx = signal.stft(data[col], nperseg=STFT_PARAMS['nperseg'], noverlap=STFT_PARAMS['noverlap'])\n",
    "        transformed_data.append(np.abs(Zxx).flatten())\n",
    "    print(\"STFT applied.\")\n",
    "    return np.array(transformed_data).T\n",
    "\n",
    "def apply_emd(data):\n",
    "    \"\"\"Apply Empirical Mode Decomposition.\"\"\"\n",
    "    transformed_data = []\n",
    "    emd = EMD()\n",
    "    for col in data.columns[1:]:\n",
    "        imfs = emd(data[col], max_imf=EMD_PARAMS['max_imf'])\n",
    "        transformed_data.append(imfs.flatten())\n",
    "    print(\"EMD applied.\")\n",
    "    return np.array(transformed_data).T\n",
    "\n",
    "def apply_pca(data):\n",
    "    \"\"\"Apply Principal Component Analysis.\"\"\"\n",
    "    pca = PCA(n_components=PCA_PARAMS['n_components'])\n",
    "    transformed_data = pca.fit_transform(data.iloc[:, 1:])\n",
    "    print(\"PCA applied.\")\n",
    "    return transformed_data\n",
    "\n",
    "def apply_ica(data):\n",
    "    \"\"\"Apply Independent Component Analysis.\"\"\"\n",
    "    ica = FastICA(n_components=ICA_PARAMS['n_components'])\n",
    "    transformed_data = ica.fit_transform(data.iloc[:, 1:])\n",
    "    print(\"ICA applied.\")\n",
    "    return transformed_data\n",
    "\n",
    "def apply_wpt(data):\n",
    "    \"\"\"Apply Wavelet Packet Transform.\"\"\"\n",
    "    transformed_data = []\n",
    "    for col in data.columns[1:]:\n",
    "        wp = pywt.WaveletPacket(data[col], wavelet=WPT_PARAMS['wavelet'], maxlevel=WPT_PARAMS['level'])\n",
    "        coeffs = [node.data for node in wp.get_level(WPT_PARAMS['level'], 'natural')]\n",
    "        transformed_data.append(np.concatenate(coeffs))\n",
    "    print(\"WPT applied.\")\n",
    "    return np.array(transformed_data).T\n",
    "\n",
    "def apply_s_transform(data):\n",
    "    \"\"\"Apply S-transform.\"\"\"\n",
    "    transformed_data = []\n",
    "    for col in data.columns[1:]:\n",
    "        f, t, st = signal.stft(data[col], nperseg=256)\n",
    "        transformed_data.append(np.abs(st).flatten())\n",
    "    print(\"S-transform applied.\")\n",
    "    return np.array(transformed_data).T\n",
    "\n",
    "def apply_mfcc(data):\n",
    "    \"\"\"Apply Mel-Frequency Cepstral Coefficients.\"\"\"\n",
    "    transformed_data = []\n",
    "    for col in data.columns[1:]:\n",
    "        mfccs = mfcc(data[col].values, samplerate=1000, numcep=MFCC_PARAMS['n_mfcc'])\n",
    "        transformed_data.append(mfccs.flatten())\n",
    "    print(\"MFCC applied.\")\n",
    "    return np.array(transformed_data).T\n",
    "\n",
    "def extract_features(data):\n",
    "    \"\"\"Extract statistical features.\"\"\"\n",
    "    features = []\n",
    "    for col in data.columns[1:]:\n",
    "        col_data = data[col]\n",
    "        features.append([\n",
    "            np.mean(col_data),\n",
    "            np.std(col_data),\n",
    "            skew(col_data),\n",
    "            kurtosis(col_data),\n",
    "            np.max(col_data),\n",
    "            np.min(col_data)\n",
    "        ])\n",
    "    print(\"Statistical features extracted.\")\n",
    "    return np.array(features).T\n",
    "\n",
    "def save_all_transformed_data(original_data, transformed_data_dict, output_file):\n",
    "    \"\"\"Save all transformed data with original labels into a single Excel file.\"\"\"\n",
    "    combined_data = original_data.copy()\n",
    "    for method, data in transformed_data_dict.items():\n",
    "        df = pd.DataFrame(data, columns=[f\"{method}_{i}\" for i in range(data.shape[1])])\n",
    "        combined_data = pd.concat([combined_data, df], axis=1)\n",
    "    \n",
    "    # Save combined data to a single sheet\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        combined_data.to_excel(writer, sheet_name='Combined_Data', index=False)\n",
    "        \n",
    "        # Save parameters\n",
    "        params_df = pd.DataFrame([\n",
    "            {'Transform': 'DWT', 'Parameters': str(DWT_PARAMS)},\n",
    "            {'Transform': 'STFT', 'Parameters': str(STFT_PARAMS)},\n",
    "            {'Transform': 'PCA', 'Parameters': str(PCA_PARAMS)},\n",
    "            {'Transform': 'ICA', 'Parameters': str(ICA_PARAMS)},\n",
    "            {'Transform': 'WPT', 'Parameters': str(WPT_PARAMS)},\n",
    "            {'Transform': 'MFCC', 'Parameters': str(MFCC_PARAMS)}\n",
    "        ])\n",
    "        params_df.to_excel(writer, sheet_name='Transform_Parameters', index=False)\n",
    "    \n",
    "    print(f\"All transformed data saved to {output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = load_data('cleaned_dataset.csv')\n",
    "    data = preprocess_data(data)\n",
    "    \n",
    "    # Apply transformations\n",
    "    transformed_data = {\n",
    "        'DWT': apply_dwt(data),\n",
    "        'STFT': apply_stft(data),\n",
    "        'PCA': apply_pca(data),\n",
    "        'ICA': apply_ica(data),\n",
    "        'WPT': apply_wpt(data),\n",
    "        'S_Transform': apply_s_transform(data),\n",
    "        'MFCC': apply_mfcc(data),\n",
    "        # 'Statistical': extract_features(data)\n",
    "    }\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"transformed_data_{timestamp}.xlsx\"\n",
    "    \n",
    "    # Save all transformed data\n",
    "    save_all_transformed_data(data, transformed_data, output_file)\n",
    "    \n",
    "    print(\"All transformations applied and saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
